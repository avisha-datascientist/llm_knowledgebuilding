{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-03T06:52:17.438133Z","iopub.execute_input":"2024-01-03T06:52:17.438747Z","iopub.status.idle":"2024-01-03T06:52:17.970353Z","shell.execute_reply.started":"2024-01-03T06:52:17.438695Z","shell.execute_reply":"2024-01-03T06:52:17.968947Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport urllib\nimport xml.etree.ElementTree as ET\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom collections import Counter\nimport string\nfrom scipy import spatial","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:50:25.839577Z","iopub.execute_input":"2024-01-03T10:50:25.840209Z","iopub.status.idle":"2024-01-03T10:50:25.848308Z","shell.execute_reply.started":"2024-01-03T10:50:25.840165Z","shell.execute_reply":"2024-01-03T10:50:25.846926Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"markdown","source":"Solution Steps:\n\n* Data Collection\n* Preprocessing: Preprocessed the collected data by removing stop words, punctuation.\n* Embeddings: Use OpenAI embeddings to create vector representations of title and summary together. \n* Question Answering: Develop a question-answering interface using a search-ask approach and GPT-3.5 openai API to generate answers.","metadata":{}},{"cell_type":"markdown","source":"# Data Collection","metadata":{}},{"cell_type":"code","source":"def fetch_papers():\n\n    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n    response = urllib.request.urlopen(url)\n    data = response.read().decode('utf-8')\n    root = ET.fromstring(data)\n\n    papers_list = []\n\n    for entry in root.findall('{http://www.w3.org/2005/Atom}entry' ):\n        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n        paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n        papers_list.append(paper_info)\n\n    return papers_list","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:41:04.949389Z","iopub.execute_input":"2024-01-03T07:41:04.949839Z","iopub.status.idle":"2024-01-03T07:41:04.958869Z","shell.execute_reply.started":"2024-01-03T07:41:04.949807Z","shell.execute_reply":"2024-01-03T07:41:04.957534Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data = fetch_papers()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:41:08.416973Z","iopub.execute_input":"2024-01-03T07:41:08.417421Z","iopub.status.idle":"2024-01-03T07:41:09.001668Z","shell.execute_reply.started":"2024-01-03T07:41:08.417385Z","shell.execute_reply":"2024-01-03T07:41:09.000417Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"text_df = pd.DataFrame(columns=['Title','Summary'])","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:28:04.522565Z","iopub.execute_input":"2024-01-03T08:28:04.523067Z","iopub.status.idle":"2024-01-03T08:28:04.535590Z","shell.execute_reply.started":"2024-01-03T08:28:04.523001Z","shell.execute_reply":"2024-01-03T08:28:04.533926Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"title = []\nsummary = []\nfor each in data:\n    res = each.split('\\n', 1)\n    title.append(res[0].split(':')[1].strip())\n    summary.append(res[1].split(':')[1].strip())","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:34:45.093847Z","iopub.execute_input":"2024-01-03T08:34:45.094257Z","iopub.status.idle":"2024-01-03T08:34:45.101327Z","shell.execute_reply.started":"2024-01-03T08:34:45.094226Z","shell.execute_reply":"2024-01-03T08:34:45.100180Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"len(title) == len(summary)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:35:04.666839Z","iopub.execute_input":"2024-01-03T08:35:04.667274Z","iopub.status.idle":"2024-01-03T08:35:04.676008Z","shell.execute_reply.started":"2024-01-03T08:35:04.667239Z","shell.execute_reply":"2024-01-03T08:35:04.674270Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"text_df['Title'] = title\ntext_df['Summary'] = summary","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:36:04.461950Z","iopub.execute_input":"2024-01-03T08:36:04.462395Z","iopub.status.idle":"2024-01-03T08:36:04.481995Z","shell.execute_reply.started":"2024-01-03T08:36:04.462361Z","shell.execute_reply":"2024-01-03T08:36:04.480550Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Remove \\n\ntext_df['Summary'] = text_df['Summary'].replace(re.compile(r'[\\n\\r\\t]'), '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:39:27.041119Z","iopub.execute_input":"2024-01-03T08:39:27.041589Z","iopub.status.idle":"2024-01-03T08:39:27.052105Z","shell.execute_reply.started":"2024-01-03T08:39:27.041538Z","shell.execute_reply":"2024-01-03T08:39:27.051152Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"corpus = ''.join(text_df['Summary'])\ndata = Counter(corpus.split())\nword_count = pd.DataFrame({'words':list(data.keys()), 'count':list(data.values())})\nword_count['word_length'] = word_count['words'].apply(lambda x: len(x))\nword_count = word_count.sort_values(by='word_length', ascending=False).reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:46:06.783954Z","iopub.execute_input":"2024-01-03T08:46:06.784363Z","iopub.status.idle":"2024-01-03T08:46:06.807043Z","shell.execute_reply.started":"2024-01-03T08:46:06.784332Z","shell.execute_reply":"2024-01-03T08:46:06.805784Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"word_count","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:46:12.324246Z","iopub.execute_input":"2024-01-03T08:46:12.324737Z","iopub.status.idle":"2024-01-03T08:46:12.342065Z","shell.execute_reply.started":"2024-01-03T08:46:12.324691Z","shell.execute_reply":"2024-01-03T08:46:12.340751Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"      index                                 words  count  word_length\n0       883  CPU\\textsuperscript{\\textregistered}      1           36\n1      1102     instruction-followingcapabilities      1           33\n2      3316       exceedingperformance.Generative      1           31\n3      2761        parameter-efficientfine-tuning      1           30\n4      2214        duringdeployment.Text-to-music      1           30\n...     ...                                   ...    ...          ...\n3397     45                                     a    152            1\n3398   2736                                     9      1            1\n3399    719                                     1      4            1\n3400   1855                                     &      1            1\n3401   1875                                     -      5            1\n\n[3402 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>words</th>\n      <th>count</th>\n      <th>word_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>883</td>\n      <td>CPU\\textsuperscript{\\textregistered}</td>\n      <td>1</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1102</td>\n      <td>instruction-followingcapabilities</td>\n      <td>1</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3316</td>\n      <td>exceedingperformance.Generative</td>\n      <td>1</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2761</td>\n      <td>parameter-efficientfine-tuning</td>\n      <td>1</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2214</td>\n      <td>duringdeployment.Text-to-music</td>\n      <td>1</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3397</th>\n      <td>45</td>\n      <td>a</td>\n      <td>152</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3398</th>\n      <td>2736</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3399</th>\n      <td>719</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3400</th>\n      <td>1855</td>\n      <td>&amp;</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3401</th>\n      <td>1875</td>\n      <td>-</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3402 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"eng_stopwords = set(stopwords.words('english'))\nprint(eng_stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:47:49.185111Z","iopub.execute_input":"2024-01-03T08:47:49.185900Z","iopub.status.idle":"2024-01-03T08:47:49.201739Z","shell.execute_reply.started":"2024-01-03T08:47:49.185850Z","shell.execute_reply":"2024-01-03T08:47:49.200705Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"{'in', 'no', 'the', 'ourselves', 'being', 'a', 'only', 'ma', 'or', 'my', \"wouldn't\", 'at', 'just', 'who', 'than', 'your', 'where', 'weren', 'own', 'this', 'hadn', 'by', \"doesn't\", 'are', 'myself', 'yours', 'there', \"isn't\", \"aren't\", 'while', 'about', 'will', 'which', 'doing', 'again', 'needn', 'i', 'shouldn', 'be', \"don't\", 'through', 'me', 'any', 'on', \"didn't\", 'wasn', 'against', 'whom', 'off', 'under', \"couldn't\", 'that', 'his', 'not', 'theirs', 'more', 'had', 'over', \"you'll\", 'because', 'all', 'having', 'him', 'shan', \"she's\", 'was', 'did', 'can', 'mustn', 'hasn', 'until', 'an', 'then', 'y', \"haven't\", 'she', 'of', 'above', 'to', 'wouldn', 'but', 'once', 'as', 's', 'don', 'll', 'were', 'here', 'itself', \"hasn't\", 'those', 'these', \"hadn't\", 'such', 'its', 'themselves', 'doesn', 'is', 'himself', 'has', 'they', 'didn', 'we', 'herself', 'few', 'further', 've', 'most', 'mightn', 'between', 'down', 're', 'before', 'other', 'won', 'below', 'them', 'am', 'does', 'if', \"should've\", 'you', 'hers', 'haven', 'into', 'isn', 'yourselves', 'nor', \"shouldn't\", 'their', 'same', 'o', 'with', 'how', 'very', 'ain', 'our', \"mustn't\", 'some', 'from', 'do', \"it's\", 'yourself', 'after', 'been', 'when', 'now', 'couldn', \"won't\", 'aren', 'out', 'each', 'so', \"mightn't\", \"that'll\", 'up', 'it', 'why', 'ours', 'both', 'should', \"weren't\", 'for', \"wasn't\", 'd', \"you'd\", \"you're\", 'have', 'during', 'what', 'he', 'her', \"shan't\", 'too', 't', \"you've\", \"needn't\", 'm', 'and'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(text):\n    # lower case\n    text = text.lower()\n    # remove punctuations\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # remove non roman\n    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)  \n    return text","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:48:21.223684Z","iopub.execute_input":"2024-01-03T08:48:21.224112Z","iopub.status.idle":"2024-01-03T08:48:21.230590Z","shell.execute_reply.started":"2024-01-03T08:48:21.224079Z","shell.execute_reply":"2024-01-03T08:48:21.229535Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"llm_text = text_df.copy()\nllm_text['Summary'] = llm_text['Summary'].map(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:50:01.545295Z","iopub.execute_input":"2024-01-03T08:50:01.545746Z","iopub.status.idle":"2024-01-03T08:50:01.562408Z","shell.execute_reply.started":"2024-01-03T08:50:01.545712Z","shell.execute_reply":"2024-01-03T08:50:01.561428Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Embeddings using text-embedding-ada-002","metadata":{}},{"cell_type":"code","source":"# can be removed and api_key can be used directly on personal computer\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"openai_api_key\")","metadata":{"execution":{"iopub.status.busy":"2024-01-03T11:14:13.459244Z","iopub.execute_input":"2024-01-03T11:14:13.459766Z","iopub.status.idle":"2024-01-03T11:14:13.614839Z","shell.execute_reply.started":"2024-01-03T11:14:13.459725Z","shell.execute_reply":"2024-01-03T11:14:13.613404Z"},"trusted":true},"execution_count":182,"outputs":[]},{"cell_type":"code","source":"def get_embeddings_from(llm_text, model):\n    '''Returns an embedding list for a concrete data. '''\n    final_embeddings = []\n    for i in range(0,len(llm_text)):\n        inputText = llm_text.loc[i,'Title'] + ' ' + llm_text.loc[i,'Summary']\n        url = f\"https://api.openai.com/v1/embeddings\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n\n        # Convert the text to a JSON payload\n        payload = {\n            \"input\": inputText,\n            \"model\": \"text-embedding-ada-002\"\n        }\n\n        #  API request\n        response = requests.post(url, headers=headers, json=payload, data=json.dumps(payload))\n        # Check the response status code\n        if response.status_code == 200:\n            # Parse the response JSON\n            data = response.json()['data'][0]\n            embeddings = data['embedding']\n            final_embeddings.append(embeddings)\n        else:\n            print(\"Failed to obtain embeddings\")\n        \n    return final_embeddings\n\ndef extract_embeddings():\n    train_embeddings = get_embeddings_from(llm_text, 'ada')\n        \n    return train_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:16:52.331302Z","iopub.execute_input":"2024-01-03T10:16:52.331842Z","iopub.status.idle":"2024-01-03T10:16:52.342545Z","shell.execute_reply.started":"2024-01-03T10:16:52.331801Z","shell.execute_reply":"2024-01-03T10:16:52.341268Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"embeddings = extract_embeddings()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:16:55.824875Z","iopub.execute_input":"2024-01-03T10:16:55.825311Z","iopub.status.idle":"2024-01-03T10:17:09.839303Z","shell.execute_reply.started":"2024-01-03T10:16:55.825276Z","shell.execute_reply":"2024-01-03T10:17:09.837951Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"# Search-Ask approach","metadata":{}},{"cell_type":"markdown","source":"Based on: https://cookbook.openai.com/examples/question_answering_using_embeddings","metadata":{}},{"cell_type":"code","source":"llm_text['Embeddings'] = embeddings\nllm_text['Title_Summary'] = llm_text['Title'] + ' ' + llm_text['Summary']","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:39:39.808173Z","iopub.execute_input":"2024-01-03T10:39:39.808663Z","iopub.status.idle":"2024-01-03T10:39:39.822684Z","shell.execute_reply.started":"2024-01-03T10:39:39.808627Z","shell.execute_reply":"2024-01-03T10:39:39.821498Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"llm_text = llm_text.drop('Title', axis=1)\nllm_text = llm_text.drop('Summary', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:40:59.945956Z","iopub.execute_input":"2024-01-03T10:40:59.946400Z","iopub.status.idle":"2024-01-03T10:40:59.954384Z","shell.execute_reply.started":"2024-01-03T10:40:59.946368Z","shell.execute_reply":"2024-01-03T10:40:59.953043Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"def strings_ranked_by_relatedness(\n    query,\n    df, top_n,\n    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y)) -> tuple[list[str], list[float]]:\n    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n    url = f\"https://api.openai.com/v1/embeddings\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Converted text to a JSON payload\n    payload = {\n        \"input\": query,\n        \"model\": \"text-embedding-ada-002\"\n    }\n    \n    response = requests.post(url, headers=headers, json=payload, data=json.dumps(payload))\n    # Parsed the response JSON\n    data = response.json()['data'][0]\n    query_embedding = data['embedding']\n    strings_and_relatednesses = [\n        (row[\"Title_Summary\"], relatedness_fn(query_embedding, row[\"Embeddings\"]))\n        for i, row in df.iterrows()\n    ]\n    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n    strings, relatednesses = zip(*strings_and_relatednesses)\n    return strings[:top_n], relatednesses[:top_n]","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:50:02.878675Z","iopub.execute_input":"2024-01-03T10:50:02.879115Z","iopub.status.idle":"2024-01-03T10:50:02.891098Z","shell.execute_reply.started":"2024-01-03T10:50:02.879081Z","shell.execute_reply":"2024-01-03T10:50:02.889600Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"strings, relatednesses = strings_ranked_by_relatedness(\"Llama-2 required memory\", llm_text, top_n=5)\nfor string, relatedness in zip(strings, relatednesses):\n    print(f\"{relatedness=:.3f}\")\n    display(string)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T10:50:32.969125Z","iopub.execute_input":"2024-01-03T10:50:32.969601Z","iopub.status.idle":"2024-01-03T10:50:33.245867Z","shell.execute_reply.started":"2024-01-03T10:50:32.969554Z","shell.execute_reply":"2024-01-03T10:50:33.244627Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"relatedness=0.847\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Llama 2 in this work we develop and release llama 2 a collection of pretrained andfinetuned large language models llms ranging in scale from 7 billion to 70billion parameters our finetuned llms called llama 2chat are optimized fordialogue use cases our models outperform opensource chat models on mostbenchmarks we tested and based on our human evaluations for helpfulness andsafety may be a suitable substitute for closedsource models we provide adetailed description of our approach to finetuning and safety improvements ofllama 2chat in order to enable the community to build on our work andcontribute to the responsible development of llms'"},"metadata":{}},{"name":"stdout","text":"relatedness=0.834\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'LLAMA the performance gap between cpu and memory widens continuously choosing thebest memory layout for each hardware architecture is increasingly important asmore and more programs become memory bound for portable codes that run acrossheterogeneous hardware architectures the choice of the memory layout for datastructures is ideally decoupled from the rest of a program this can beaccomplished via a zeroruntimeoverhead abstraction layer underneath whichmemory layouts can be freely exchanged  we present the lowlevel abstraction of memory access llama a c librarythat provides such a data structure abstraction layer with exampleimplementations for multidimensional arrays of nested structured data llamaprovides fully c compliant methods for defining and switching custom memorylayouts for userdefined data types the library is extensible with thirdpartyallocators  providing two closetolife examples we show that the llamagenerated aosarray of structs and soa struct of arrays layouts produce identical codewith the same performance characteristics as manually written data structuresintegrations into the spec cputextsuperscripttextregistered lbm benchmarkand the particleincell simulation picongpu demonstrate llamas abilities inrealworld applications llamas layoutaware copy routines can significantlyspeed up transfer and reshuffling of data between layouts compared with naiveelementwise copying  llama provides a novel tool for the development of highperformance capplications in a heterogeneous environment'"},"metadata":{}},{"name":"stdout","text":"relatedness=0.824\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Code Llama we release code llama a family of large language models for code based onllama 2 providing stateoftheart performance among open models infillingcapabilities support for large input contexts and zeroshot instructionfollowing ability for programming tasks we provide multiple flavors to cover awide range of applications'"},"metadata":{}},{"name":"stdout","text":"relatedness=0.805\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Baby Llama we present our submission to the babylm challenge whose goal was to improvethe sample efficiency of language models we trained an ensemble consisting ofa gpt2 and small llama models on the developmentallyplausible 10mwordbabylm dataset then distilled it into a small 58mparameter llama modelwhich exceeds in performance both of its teachers as well as a similar modeltrained without distillation this suggests that distillation can not onlyretain the full performance of the teacher model when the latter is trained ona sufficiently small dataset it can exceed it and lead to significantlybetter performance than direct training'"},"metadata":{}},{"name":"stdout","text":"relatedness=0.805\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'BadLlama llama 2chat is a collection of large language models that meta developed andreleased to the public while meta finetuned llama 2chat to refuse to outputharmful content we hypothesize that public access to model weights enables badactors to cheaply circumvent llama 2chats safeguards and weaponize llama 2scapabilities for malicious purposes we demonstrate that it is possible toeffectively undo the safety finetuning from llama 2chat 13b with less than200 while retaining its general capabilities our results demonstrate thatsafetyfine tuning is ineffective at preventing misuse when model weights arereleased publicly given that future models will likely have much greaterability to cause harm at scale it is essential that ai developers addressthreats from finetuning when considering whether to publicly release theirmodel weights'"},"metadata":{}}]},{"cell_type":"code","source":"#Return a message for GPT, with relevant source texts pulled from a dataframe.\ndef query_message(\n    query: str,\n    df: pd.DataFrame\n) -> str:\n    strings, relatednesses = strings_ranked_by_relatedness(query, llm_text, top_n=15)\n    question = f\"\\n\\nQuestion: {query}\"\n    message = ''\n    for string in strings:\n        message += string\n    return message + question\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T11:10:02.559155Z","iopub.execute_input":"2024-01-03T11:10:02.559607Z","iopub.status.idle":"2024-01-03T11:10:02.567534Z","shell.execute_reply.started":"2024-01-03T11:10:02.559571Z","shell.execute_reply":"2024-01-03T11:10:02.565994Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"#Answers a query using GPT 3.5 and a dataframe of relevant texts and embeddings.\ndef ask(\n    query: str,\n    df,\n    token_budget = 4096 - 500,\n    print_message = False,\n) -> str:\n    message = query_message(query, llm_text)\n    if print_message:\n        print(message)\n   \n    url = f\"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    # Converted the text to a JSON payload\n    payload = {\n        \"model\": \"gpt-3.5-turbo\",\n         \"messages\": [{\"role\": \"system\", \"content\": \"You answer questions about the 2022 Winter Olympics.\"},\n        {\"role\": \"user\", \"content\": message},],\n         \"temperature\": 0.7\n    }\n    \n    response = requests.post(url, headers=headers, json=payload, data=json.dumps(payload))\n    response_message = response.json()['choices']\n    msg = response_message[0]['message']\n    content = msg['content']\n    return content\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T11:11:02.557806Z","iopub.execute_input":"2024-01-03T11:11:02.558244Z","iopub.status.idle":"2024-01-03T11:11:02.569076Z","shell.execute_reply.started":"2024-01-03T11:11:02.558211Z","shell.execute_reply":"2024-01-03T11:11:02.567503Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"ask('For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?', llm_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T11:11:06.757120Z","iopub.execute_input":"2024-01-03T11:11:06.757578Z","iopub.status.idle":"2024-01-03T11:11:17.509253Z","shell.execute_reply.started":"2024-01-03T11:11:06.757540Z","shell.execute_reply":"2024-01-03T11:11:17.508079Z"},"trusted":true},"execution_count":181,"outputs":[{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"'Based on the given text, it is not explicitly mentioned for which tasks Llama-2 has been used successfully. However, it is stated that the finetuned Llama-2 models, called Llama 2chat, outperform opensource chat models on most benchmarks. Therefore, it can be inferred that Llama-2 has been used successfully for dialogue-based tasks.\\n\\nPromising areas of application for Llama-2 could include:\\n1. Dialogue systems: Llama 2chat is specifically optimized for dialogue use cases and has shown superior performance compared to opensource chat models.\\n2. Text generation: Large language models like Llama-2 have demonstrated significant progress in text generation tasks, which can be applied to various domains.\\n3. Code generation: Llama 2, specifically the Code Llama variant, provides state-of-the-art performance for code-based applications, including support for large input contexts and instruction following ability for programming tasks.\\n4. Legal domain: Lawyer Llama is a model based on Llama-2 that aims to adapt large language models to specific domains like law, leveraging domain-specific knowledge to resolve legal problems.\\n\\nIt is important to note that the given text is not exhaustive, and further research and experimentation may identify additional successful applications and promising areas for Llama-2.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}